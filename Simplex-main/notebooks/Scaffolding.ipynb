{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus finder tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import sys\n",
    "from random import random\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import pickle as pkl\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import captum.attr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_influence_functions as ptif\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from simplexai.explainers.nearest_neighbours import NearNeighLatent\n",
    "from simplexai.explainers.representer import Representer\n",
    "from simplexai.explainers.simplex import Simplex\n",
    "from simplexai.explainers.contex import Contex\n",
    "from simplexai.models.image_recognition import MnistClassifier\n",
    "from simplexai.utils.schedulers import ExponentialScheduler\n",
    "\n",
    "\n",
    "\n",
    "from simplexai.experiments.time_series_prostate_cancer import (\n",
    "    TimeSeriesProstateCancerDataset,\n",
    "    load_time_series_prostate_cancer,\n",
    ")\n",
    "from simplexai.explainers.simplex import Simplex\n",
    "from simplexai.models.recurrent_neural_net import MortalityGRU\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the test_inputs: \n",
      "torch.Size([1, 1, 28, 28])\n",
      "We wish for it to be in this shape: \n",
      "torch.Size([10, 1, 28, 28])\n",
      "10\n",
      "Here is the new tensor: \n",
      "torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "def load_mnist(\n",
    "    batch_size: int, train: bool, subset_size=None, shuffle: bool = True\n",
    ") -> DataLoader:\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        \"./data/\",\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    if subset_size:\n",
    "        dataset = torch.utils.data.Subset(\n",
    "            dataset, torch.randperm(len(dataset))[:subset_size]\n",
    "        )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "corpus_size = 10\n",
    "\n",
    "corpus_loader = load_mnist(corpus_size, train=True)\n",
    "corpus_examples = enumerate(corpus_loader)\n",
    "_, (corpus_inputs, corpus_target) = next(corpus_examples)\n",
    "corpus_inputs = corpus_inputs.detach()\n",
    "test_examples = enumerate(test_loader)\n",
    "_, (test_inputs, test_target) = next(test_examples)\n",
    "test_inputs = test_inputs[test_id : test_id + 1].detach()\n",
    "test_data = test_inputs\n",
    "\n",
    "print(\"This is the test_inputs: \")\n",
    "print(test_inputs.shape)\n",
    "\n",
    "print(\"We wish for it to be in this shape: \")\n",
    "print(corpus_inputs.shape)\n",
    "\n",
    "new_tensor = [test_inputs] * corpus_size\n",
    "new_tensor = torch.cat(new_tensor)\n",
    "print(len(new_tensor))\n",
    "\n",
    "print(\"Here is the new tensor: \")\n",
    "print(new_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "target label (test point label) :  \n",
      "tensor(3)\n",
      "corpus labels :  \n",
      "tensor([8, 5, 0, 3, 1, 1, 7, 1, 6, 8, 0, 1, 4, 2, 5, 7, 9, 5, 7, 7, 9, 0, 0, 1,\n",
      "        1, 5, 5, 9, 3, 6, 0, 1, 4, 1, 0, 1, 1, 8, 8, 1, 1, 8, 2, 6, 0, 0, 2, 2,\n",
      "        2, 9, 3, 6, 1, 6, 5, 9, 2, 8, 0, 3, 9, 9, 8, 6, 0, 8, 4, 8, 4, 0, 3, 1,\n",
      "        1, 3, 5, 7, 8, 9, 1, 0, 1, 7, 7, 9, 0, 2, 9, 0, 2, 0, 7, 2, 1, 3, 6, 8,\n",
      "        4, 6, 3, 7, 8, 3, 3, 7, 9, 5, 4, 9, 7, 1, 5, 8, 7, 0, 5, 3, 6, 0, 0, 2,\n",
      "        6, 5, 1, 8, 5, 0, 3, 7, 1, 1, 5, 7, 9, 5, 0, 2, 5, 8, 4, 7, 7, 7, 5, 1,\n",
      "        9, 1, 0, 4, 7, 3, 1, 8, 3, 6, 8, 8, 7, 4, 4, 9, 6, 9, 2, 3, 3, 2, 3, 2,\n",
      "        8, 2, 1, 8, 6, 9, 3, 7, 1, 9, 6, 8, 9, 8, 3, 9, 0, 9, 1, 1, 8, 0, 0, 5,\n",
      "        9, 1, 2, 9, 0, 6, 7, 5, 0, 7, 3, 1, 2, 7, 9, 6, 5, 3, 2, 9, 3, 8, 5, 8,\n",
      "        4, 4, 4, 9, 9, 6, 9, 0, 0, 5, 0, 9, 5, 3, 0, 1, 0, 4, 4, 4, 1, 4, 7, 9,\n",
      "        7, 2, 6, 2, 2, 7, 9, 3, 7, 0, 0, 1, 1, 4, 8, 3, 7, 7, 0, 7, 1, 9, 0, 4,\n",
      "        5, 3, 8, 9, 2, 5, 9, 4, 9, 1, 2, 5, 0, 0, 3, 9, 4, 7, 4, 6, 3, 0, 9, 6,\n",
      "        5, 1, 6, 7, 9, 0, 0, 0, 0, 5, 6, 3, 0, 1, 9, 0, 0, 5, 8, 2, 1, 7, 8, 0,\n",
      "        4, 4, 0, 4, 2, 5, 3, 7, 8, 1, 6, 7, 8, 0, 8, 7, 5, 5, 6, 5, 0, 9, 3, 7,\n",
      "        5, 1, 9, 2, 7, 8, 5, 1, 9, 3, 7, 5, 8, 2, 4, 8, 2, 9, 6, 7, 8, 6, 4, 0,\n",
      "        2, 6, 4, 6, 1, 7, 3, 8, 3, 2, 8, 6, 7, 0, 1, 8, 8, 3, 7, 4, 7, 8, 6, 8,\n",
      "        0, 6, 6, 7, 2, 9, 8, 4, 2, 3, 9, 3, 9, 3, 8, 6, 1, 9, 2, 0, 5, 7, 0, 0,\n",
      "        4, 0, 8, 3, 1, 9, 3, 4, 7, 4, 0, 5, 0, 3, 2, 1, 3, 3, 6, 3, 7, 9, 3, 4,\n",
      "        7, 0, 7, 4, 0, 9, 7, 1, 8, 1, 5, 3, 6, 0, 1, 1, 5, 3, 8, 9, 3, 0, 5, 5,\n",
      "        8, 6, 2, 3, 7, 3, 1, 6, 1, 1, 0, 7, 4, 6, 8, 1, 3, 3, 0, 8, 7, 0, 0, 5,\n",
      "        0, 1, 3, 3, 8, 9, 0, 6, 5, 0, 4, 9, 0, 2, 5, 7, 2, 5, 4, 9])\n",
      "cf potentials :\n",
      "[1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "CF indices\n",
      "[  0   1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56\n",
      "  57  58  60  61  62  63  64  65  66  67  68  69  71  72  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  94  95  96\n",
      "  97  99 100 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118\n",
      " 119 120 121 122 123 124 125 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 150 151 153 154 155 156 157\n",
      " 158 159 160 161 162 165 167 168 169 170 171 172 173 175 176 177 178 179\n",
      " 180 181 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 203 204 205 206 207 208 210 211 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 248 249 250 251 252 253 254 256 257 258\n",
      " 259 260 261 262 263 264 266 267 268 269 270 271 272 273 274 275 276 277\n",
      " 279 280 281 282 283 285 286 287 288 289 290 291 292 293 294 295 296 297\n",
      " 298 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 335 336\n",
      " 337 338 339 340 341 342 343 344 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 367 369 370 371 372 373 374 375\n",
      " 376 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 394 396\n",
      " 398 399 400 401 402 403 404 405 406 407 408 409 410 412 413 415 416 417\n",
      " 418 419 420 422 423 426 428 429 431 432 433 434 435 436 437 438 439 440\n",
      " 441 442 444 445 446 447 448 450 451 453 454 455 456 457 458 460 462 463\n",
      " 464 465 466 467 468 469 470 471 474 475 476 477 478 479 480 481 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499]\n",
      "\n",
      " \n",
      " Now we print the second counterfactual's index \n",
      " \n",
      "1\n",
      "CFs labels\n",
      "torch.Size([446, 1, 28, 28])\n",
      "number cfs\n",
      "446\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test_inputs: of dimension  \n",
      "torch.Size([1, 28, 28])\n",
      "flattened: \n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([784])\n",
      "\n",
      "\n",
      "\n",
      "Here are the counterfactual distances to test_point\n",
      "[35.23031235 29.53841972 35.59017181 36.17677307 37.08761215 38.71287155\n",
      " 38.24267197 31.90040588 32.84750366 37.98460007 35.23236465 39.31375885\n",
      " 34.00550842 34.62432861 38.74211884 35.33889008 36.69736481 39.96765518\n",
      " 38.85861206 42.13631439 37.08204269 33.79597855 36.52971268 37.33452988\n",
      " 39.73757553 33.87437439 40.8145752  33.20681381 37.83722687 36.15736771\n",
      " 37.93192673 37.27575684 34.73915482 36.34217453 35.99240494 36.4251442\n",
      " 37.25371933 35.68764877 36.36346436 37.82570267 37.84041977 37.28086472\n",
      " 35.51856613 28.86897659 36.58997726 38.31240845 32.96755219 38.32867813\n",
      " 37.75910187 37.04771423 34.87967682 35.85559082 36.04316711 38.69535446\n",
      " 35.54736328 34.53704834 38.27359772 39.33121872 36.79703522 36.61355209\n",
      " 35.51678848 38.21958542 41.54620361 38.26003265 43.1181488  36.27412415\n",
      " 37.51659775 36.26755142 29.10256386 39.88454056 34.81059265 37.72615433\n",
      " 35.68806458 34.61675262 36.02231598 40.68218994 37.39176178 37.35102081\n",
      " 30.32872963 39.6961174  40.25117111 40.94390488 32.38644791 40.34925079\n",
      " 38.36487579 34.6178894  37.27362823 36.30302429 33.33263779 37.70384598\n",
      " 39.51364136 39.1324234  36.76618576 40.47848892 37.24814224 37.0124321\n",
      " 39.40977478 41.36415482 36.98642731 36.7523613  37.17443848 32.79039001\n",
      " 40.22599792 35.44888687 30.5942173  37.92653656 31.99380684 37.45481491\n",
      " 37.56476974 39.02440262 33.63620377 36.92274094 36.74212646 28.47343826\n",
      " 32.07746124 39.86612701 37.1427803  35.93209076 27.99724007 35.11148071\n",
      " 38.64172363 36.48714828 40.30161285 36.54557037 33.15538025 34.73350143\n",
      " 35.73384476 39.08554459 39.45640564 40.05619049 40.2313118  36.16825867\n",
      " 38.28130722 37.04807281 37.3248558  38.84442139 38.96222305 36.13656998\n",
      " 34.54260635 39.07351685 39.37670517 35.26713562 38.25926971 40.03753281\n",
      " 43.59030533 40.32506561 41.531353   38.04865265 32.58104324 40.73559952\n",
      " 37.31464386 36.88612747 34.35079193 37.02806091 36.61910629 40.6426239\n",
      " 36.2102356  37.8164444  36.54525375 41.10388184 38.00413132 36.26087189\n",
      " 35.48946762 38.72006989 38.80872726 36.62226105 37.87882996 35.94439316\n",
      " 36.6971283  34.65419388 32.90668488 33.46408844 36.19868088 37.03226089\n",
      " 36.46122742 38.54016876 36.74458694 35.24763107 37.79724503 37.16070175\n",
      " 34.87207413 42.05974579 39.41518402 36.33650208 33.37932968 37.2630043\n",
      " 38.26244736 38.31490707 33.1760788  36.22986984 39.50720215 35.80544662\n",
      " 37.13960266 35.14900208 41.98827744 39.96851349 36.58018112 35.02825546\n",
      " 40.92965317 35.87358093 44.82502747 31.30267334 32.40771484 32.79002762\n",
      " 36.07868195 40.5828476  40.39222336 40.90255356 37.88102722 35.2309227\n",
      " 36.52963257 41.6842804  43.0097847  36.94677734 40.47021484 40.23833847\n",
      " 36.62571335 39.1911087  39.37755966 40.34482574 41.52217102 37.32474518\n",
      " 39.93812561 37.73678589 38.36423111 36.06295395 36.42467117 36.48356247\n",
      " 37.12334442 46.25986099 36.17831802 40.69071198 37.68188477 36.88891983\n",
      " 39.25313568 36.58575439 37.43516541 34.89465332 40.18653107 39.11231995\n",
      " 36.67258835 41.42726135 40.60445404 32.71475983 38.71047592 37.5249176\n",
      " 38.19651031 38.14728928 36.56439972 36.62195206 32.51361847 38.64187622\n",
      " 35.20489502 40.2747879  41.15556335 39.74967957 37.52255249 32.05885696\n",
      " 34.89065552 35.19344711 39.1674614  37.19885254 37.96490097 37.87176895\n",
      " 33.27235413 35.81999969 32.1942482  32.51831818 33.67180252 29.15232468\n",
      " 36.24554443 31.15757561 35.99543762 36.64163589 37.8266716  32.83605194\n",
      " 36.89191818 38.05958557 40.54332733 35.63233566 38.93853378 35.24012375\n",
      " 39.66070938 41.388237   39.7401619  34.39073563 41.16744232 37.39395523\n",
      " 39.55392456 39.20309448 36.99726105 36.21880722 38.26581192 39.31487656\n",
      " 32.78032684 35.76958847 35.4199028  43.20673752 37.75325394 31.65760231\n",
      " 36.27290344 38.26736069 33.60795212 39.23756409 37.03820801 34.94378662\n",
      " 35.91127014 37.18936157 36.19768906 38.09379959 35.32453918 32.93784714\n",
      " 35.80714035 36.57440186 39.51960754 40.13777542 36.480896   34.19202805\n",
      " 41.97676086 38.95045471 40.66930008 38.59705353 35.8667717  40.76512146\n",
      " 35.84151459 33.11893845 42.16855621 33.15987778 38.95743561 41.8551712\n",
      " 37.1921196  37.80295944 36.35375214 35.84528732 34.13033676 36.88077164\n",
      " 38.73390579 32.59207916 37.93088531 34.81104279 35.74103165 35.64527893\n",
      " 37.43917465 38.42468262 40.54027557 40.75533676 36.01147079 36.11120987\n",
      " 37.7232132  35.98188019 37.62055969 39.01832962 38.86568069 41.502491\n",
      " 39.66819    33.9432869  37.87018967 36.52412415 37.56037521 36.83394623\n",
      " 37.01025009 37.97983932 36.9025383  39.64006042 37.57266617 34.38709259\n",
      " 36.05001068 39.75875092 41.81898499 34.9607811  37.3866272  33.12234116\n",
      " 35.49317932 36.50674438 38.09980774 39.5294075  39.0308075  41.46353912\n",
      " 34.37493896 31.92140198 34.66698074 39.89935303 32.58615875 39.08175278\n",
      " 36.40483475 41.71611023 38.56272125 39.72924042 37.85945892 40.22531128\n",
      " 37.24032974 34.54636002 37.01298523 39.23991394 35.72862625 37.17217255\n",
      " 35.14726639 34.25950623 37.74880981 35.07138062 36.80006409 37.1461792\n",
      " 29.14032173 38.10493469 37.97926331 30.45978737 36.6565094  32.92110062\n",
      " 37.22739792 35.7312355  35.439888   37.8732872  36.13195419 35.94316864\n",
      " 35.64984512 36.47703171 32.67788315 37.71765518 42.35280991 36.7135849\n",
      " 35.19773102 36.47161102 35.6230278  34.59865189 39.49568939 32.67185593\n",
      " 38.88272858 39.76750183 38.45121002 37.86714554 37.08716202 37.3187027\n",
      " 38.0931778  43.68362045 31.18581009 32.41950989 37.46912766 28.22690582\n",
      " 37.54053116 35.64541626 32.37960434 39.69864655 40.76398468 33.56528091\n",
      " 41.44871521 41.36396027]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the adjacency matrix\n",
      "(501, 501)\n",
      "[[ 0.         24.08324432 36.38453674 ... 35.33808899 31.45649529\n",
      "  35.23031235]\n",
      " [24.08324432  0.         31.39158249 ... 34.378685   35.46274185\n",
      "  29.53841972]\n",
      " [36.38453674 31.39158249  0.         ... 34.94286346 40.64636993\n",
      "  35.59017181]\n",
      " ...\n",
      " [35.33808899 34.378685   34.94286346 ...  0.         34.2791214\n",
      "  41.44871521]\n",
      " [31.45649529 35.46274185 40.64636993 ... 34.2791214   0.\n",
      "  41.36396027]\n",
      " [35.23031235 29.53841972 35.59017181 ... 41.44871521 41.36396027\n",
      "   0.        ]]\n",
      "\n",
      " We have a new adjacency matrix: \n",
      "[[ 0.         24.08324432  0.         ... 35.33808899 31.45649529\n",
      "  35.23031235]\n",
      " [24.08324432  0.         31.39158249 ...  0.          0.\n",
      "  29.53841972]\n",
      " [36.38453674 31.39158249  0.         ... 34.94286346  0.\n",
      "  35.59017181]\n",
      " ...\n",
      " [ 0.         34.378685    0.         ...  0.         34.2791214\n",
      "   0.        ]\n",
      " [31.45649529 35.46274185  0.         ... 34.2791214   0.\n",
      "   0.        ]\n",
      " [35.23031235 29.53841972 35.59017181 ...  0.          0.\n",
      "   0.        ]]\n",
      "\n",
      " Shortest paths matrix: \n",
      "[[ 0.         24.08324432 52.55220795 ... 35.33808899 31.45649529\n",
      "  35.23031235]\n",
      " [24.08324432  0.         31.39158249 ... 50.556036   52.82199287\n",
      "  29.53841972]\n",
      " [36.38453674 31.39158249  0.         ... 34.94286346 59.79607773\n",
      "  35.59017181]\n",
      " ...\n",
      " [52.77457809 34.378685   53.80815887 ...  0.         34.2791214\n",
      "  59.65039062]\n",
      " [31.45649529 35.46274185 59.79607773 ... 34.2791214   0.\n",
      "  62.06833649]\n",
      " [35.23031235 29.53841972 35.59017181 ... 59.65039062 62.10585976\n",
      "   0.        ]]\n",
      "\n",
      " Predecessors matrix: \n",
      "[[-9999     0   489 ...     0     0     0]\n",
      " [    1 -9999     1 ...   275   292     1]\n",
      " [    2     2 -9999 ...     2   394     2]\n",
      " ...\n",
      " [   66   498   304 ... -9999   498    74]\n",
      " [  499   499   394 ...   499 -9999   425]\n",
      " [  500   500   500 ...    74   292 -9999]]\n",
      "\n",
      " Shortest distances between counterfactuals and test point:\n",
      "[35.23031235 29.53841972 35.59017181 36.17677307 37.08761215 56.01072693\n",
      " 46.63123322 31.90040588 32.84750366 37.98460007 35.23236465 57.35257721\n",
      " 34.00550842 34.62432861 54.5992794  35.33889008 36.69736481 59.4452076\n",
      " 58.82790756 62.91637421 37.08204269 33.79597855 36.52971268 37.33452988\n",
      " 56.71961212 33.87437439 65.04694557 33.20681381 37.83722687 36.15736771\n",
      " 37.93192673 37.27575684 34.73915482 36.34217453 35.99240494 36.4251442\n",
      " 37.25371933 35.68764877 36.36346436 37.82570267 37.84041977 37.28086472\n",
      " 35.51856613 28.86897659 36.58997726 55.00441933 32.96755219 53.65786934\n",
      " 37.75910187 37.04771423 34.87967682 35.85559082 36.04316711 55.72533607\n",
      " 35.54736328 34.53704834 56.620121   54.98911858 36.79703522 36.61355209\n",
      " 35.51678848 57.67063332 59.21166229 54.49812698 63.46576881 36.27412415\n",
      " 37.51659775 36.26755142 29.10256386 54.30308914 34.81059265 37.72615433\n",
      " 35.68806458 34.61675262 36.02231598 57.75514603 37.39176178 37.35102081\n",
      " 30.32872963 57.67126846 57.90085411 61.19890404 32.38644791 61.19095993\n",
      " 55.98100662 34.6178894  37.27362823 36.30302429 33.33263779 37.70384598\n",
      " 57.80610085 59.44029808 36.76618576 58.88163757 37.24814224 37.0124321\n",
      " 58.27449226 62.60799026 36.98642731 36.7523613  37.17443848 32.79039001\n",
      " 59.50069809 35.44888687 30.5942173  37.92653656 31.99380684 37.45481491\n",
      " 37.56476974 58.14795303 33.63620377 36.92274094 36.74212646 28.47343826\n",
      " 32.07746124 57.13910103 37.1427803  35.93209076 27.99724007 35.11148071\n",
      " 55.99185944 36.48714828 57.72437477 36.54557037 33.15538025 34.73350143\n",
      " 35.73384476 56.9852562  58.2773819  56.3780632  57.39551163 36.16825867\n",
      " 58.12484169 37.04807281 37.3248558  54.71770477 56.04755592 36.13656998\n",
      " 34.54260635 56.36137962 59.12156105 35.26713562 52.72224045 58.50995445\n",
      " 67.38428688 59.02076912 62.65059471 38.04865265 32.58104324 57.59574127\n",
      " 37.31464386 36.88612747 34.35079193 37.02806091 36.61910629 56.25392532\n",
      " 36.2102356  37.8164444  36.54525375 59.18104553 38.00413132 36.26087189\n",
      " 35.48946762 59.26938629 58.03346825 36.62226105 37.87882996 35.94439316\n",
      " 36.6971283  34.65419388 32.90668488 33.46408844 36.19868088 37.03226089\n",
      " 36.46122742 59.66990471 36.74458694 35.24763107 37.79724503 37.16070175\n",
      " 34.87207413 60.44744492 60.0171566  36.33650208 33.37932968 37.2630043\n",
      " 54.23481941 57.8068161  33.1760788  36.22986984 59.65023804 35.80544662\n",
      " 37.13960266 35.14900208 61.64069939 57.46670914 36.58018112 35.02825546\n",
      " 57.93104744 35.87358093 67.78824615 31.30267334 32.40771484 32.79002762\n",
      " 36.07868195 56.70429993 60.15719604 60.06953239 37.88102722 35.2309227\n",
      " 36.52963257 61.42320061 62.47194099 36.94677734 58.8877449  56.7618351\n",
      " 36.62571335 59.01185989 58.44726753 60.21349716 61.04843903 37.32474518\n",
      " 57.77822876 37.73678589 58.31158257 36.06295395 36.42467117 36.48356247\n",
      " 37.12334442 72.74436951 36.17831802 59.98744583 37.68188477 36.88891983\n",
      " 54.74222946 36.58575439 37.43516541 34.89465332 54.45596123 58.96582794\n",
      " 36.67258835 59.94673538 58.26125336 32.71475983 55.83578682 37.5249176\n",
      " 56.63949585 52.4043293  36.56439972 36.62195206 32.51361847 59.87528992\n",
      " 35.20489502 59.83960724 58.23130417 57.46566582 37.52255249 32.05885696\n",
      " 34.89065552 35.19344711 54.3341217  37.19885254 37.96490097 37.87176895\n",
      " 33.27235413 35.81999969 32.1942482  32.51831818 33.67180252 29.15232468\n",
      " 36.24554443 31.15757561 35.99543762 36.64163589 37.8266716  32.83605194\n",
      " 36.89191818 38.05958557 58.67530823 35.63233566 57.03458595 35.24012375\n",
      " 53.5775547  63.48937225 58.66716576 34.39073563 59.89963531 37.39395523\n",
      " 56.84783936 58.75166512 36.99726105 36.21880722 53.70030022 56.27100563\n",
      " 32.78032684 35.76958847 35.4199028  63.48639297 37.75325394 31.65760231\n",
      " 36.27290344 56.07792664 33.60795212 58.01073837 37.03820801 34.94378662\n",
      " 35.91127014 37.18936157 36.19768906 57.95788574 35.32453918 32.93784714\n",
      " 35.80714035 36.57440186 55.96542931 57.91973877 36.480896   34.19202805\n",
      " 65.29702568 56.89650726 60.44051743 59.36119843 35.8667717  56.67276001\n",
      " 35.84151459 33.11893845 61.56104469 33.15987778 57.66045952 62.15102196\n",
      " 37.1921196  37.80295944 36.35375214 35.84528732 34.13033676 36.88077164\n",
      " 58.10350418 32.59207916 37.93088531 34.81104279 35.74103165 35.64527893\n",
      " 37.43917465 52.83006096 50.96246433 58.76494026 36.01147079 36.11120987\n",
      " 37.7232132  35.98188019 37.62055969 55.06679344 56.98462105 58.59319305\n",
      " 63.34394073 33.9432869  37.87018967 36.52412415 37.56037521 36.83394623\n",
      " 37.01025009 37.97983932 36.9025383  58.92947197 37.57266617 34.38709259\n",
      " 36.05001068 61.10268211 57.40144348 34.9607811  37.3866272  33.12234116\n",
      " 35.49317932 36.50674438 53.79890442 59.30043983 56.84363365 63.46366119\n",
      " 34.37493896 31.92140198 34.66698074 63.48820114 32.58615875 55.27760887\n",
      " 36.40483475 62.57388687 57.5661602  55.91798401 37.85945892 56.26211166\n",
      " 37.24032974 34.54636002 37.01298523 59.40070343 35.72862625 37.17217255\n",
      " 35.14726639 34.25950623 37.74880981 35.07138062 36.80006409 37.1461792\n",
      " 29.14032173 57.29617119 37.97926331 30.45978737 36.6565094  32.92110062\n",
      " 37.22739792 35.7312355  35.439888   37.8732872  36.13195419 35.94316864\n",
      " 35.64984512 36.47703171 32.67788315 37.71765518 64.07261658 36.7135849\n",
      " 35.19773102 36.47161102 35.6230278  34.59865189 56.24078178 32.67185593\n",
      " 56.05433464 60.66259956 53.8809433  37.86714554 37.08716202 37.3187027\n",
      " 38.0931778  65.92631149 31.18581009 32.41950989 37.46912766 28.22690582\n",
      " 37.54053116 35.64541626 32.37960434 54.9227581  60.18688202 33.56528091\n",
      " 59.65039062 62.10585976]\n",
      "Reminder: counterfactual indices are: \n",
      "[  0   1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56\n",
      "  57  58  60  61  62  63  64  65  66  67  68  69  71  72  74  75  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  94  95  96\n",
      "  97  99 100 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118\n",
      " 119 120 121 122 123 124 125 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 150 151 153 154 155 156 157\n",
      " 158 159 160 161 162 165 167 168 169 170 171 172 173 175 176 177 178 179\n",
      " 180 181 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 203 204 205 206 207 208 210 211 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 230 231 232 233 234 235 236 237 238\n",
      " 239 240 241 242 243 244 245 246 248 249 250 251 252 253 254 256 257 258\n",
      " 259 260 261 262 263 264 266 267 268 269 270 271 272 273 274 275 276 277\n",
      " 279 280 281 282 283 285 286 287 288 289 290 291 292 293 294 295 296 297\n",
      " 298 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 335 336\n",
      " 337 338 339 340 341 342 343 344 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 367 369 370 371 372 373 374 375\n",
      " 376 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 394 396\n",
      " 398 399 400 401 402 403 404 405 406 407 408 409 410 412 413 415 416 417\n",
      " 418 419 420 422 423 426 428 429 431 432 433 434 435 436 437 438 439 440\n",
      " 441 442 444 445 446 447 448 450 451 453 454 455 456 457 458 460 462 463\n",
      " 464 465 466 467 468 469 470 471 474 475 476 477 478 479 480 481 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499]\n",
      "Reminder: counterfactual labels are: \n",
      "tensor([8, 5, 0, 1, 1, 7, 1, 6, 8, 0, 1, 4, 2, 5, 7, 9, 5, 7, 7, 9, 0, 0, 1, 1,\n",
      "        5, 5, 9, 6, 0, 1, 4, 1, 0, 1, 1, 8, 8, 1, 1, 8, 2, 6, 0, 0, 2, 2, 2, 9,\n",
      "        6, 1, 6, 5, 9, 2, 8, 0, 9, 9, 8, 6, 0, 8, 4, 8, 4, 0, 1, 1, 5, 7, 8, 9,\n",
      "        1, 0, 1, 7, 7, 9, 0, 2, 9, 0, 2, 0, 7, 2, 1, 6, 8, 4, 6, 7, 8, 7, 9, 5,\n",
      "        4, 9, 7, 1, 5, 8, 7, 0, 5, 6, 0, 0, 2, 6, 5, 1, 8, 5, 0, 7, 1, 1, 5, 7,\n",
      "        9, 5, 0, 2, 5, 8, 4, 7, 7, 7, 5, 1, 9, 1, 0, 4, 7, 1, 8, 6, 8, 8, 7, 4,\n",
      "        4, 9, 6, 9, 2, 2, 2, 8, 2, 1, 8, 6, 9, 7, 1, 9, 6, 8, 9, 8, 9, 0, 9, 1,\n",
      "        1, 8, 0, 0, 5, 9, 1, 2, 9, 0, 6, 7, 5, 0, 7, 1, 2, 7, 9, 6, 5, 2, 9, 8,\n",
      "        5, 8, 4, 4, 4, 9, 9, 6, 9, 0, 0, 5, 0, 9, 5, 0, 1, 0, 4, 4, 4, 1, 4, 7,\n",
      "        9, 7, 2, 6, 2, 2, 7, 9, 7, 0, 0, 1, 1, 4, 8, 7, 7, 0, 7, 1, 9, 0, 4, 5,\n",
      "        8, 9, 2, 5, 9, 4, 9, 1, 2, 5, 0, 0, 9, 4, 7, 4, 6, 0, 9, 6, 5, 1, 6, 7,\n",
      "        9, 0, 0, 0, 0, 5, 6, 0, 1, 9, 0, 0, 5, 8, 2, 1, 7, 8, 0, 4, 4, 0, 4, 2,\n",
      "        5, 7, 8, 1, 6, 7, 8, 0, 8, 7, 5, 5, 6, 5, 0, 9, 7, 5, 1, 9, 2, 7, 8, 5,\n",
      "        1, 9, 7, 5, 8, 2, 4, 8, 2, 9, 6, 7, 8, 6, 4, 0, 2, 6, 4, 6, 1, 7, 8, 2,\n",
      "        8, 6, 7, 0, 1, 8, 8, 7, 4, 7, 8, 6, 8, 0, 6, 6, 7, 2, 9, 8, 4, 2, 9, 9,\n",
      "        8, 6, 1, 9, 2, 0, 5, 7, 0, 0, 4, 0, 8, 1, 9, 4, 7, 4, 0, 5, 0, 2, 1, 6,\n",
      "        7, 9, 4, 7, 0, 7, 4, 0, 9, 7, 1, 8, 1, 5, 6, 0, 1, 1, 5, 8, 9, 0, 5, 5,\n",
      "        8, 6, 2, 7, 1, 6, 1, 1, 0, 7, 4, 6, 8, 1, 0, 8, 7, 0, 0, 5, 0, 1, 8, 9,\n",
      "        0, 6, 5, 0, 4, 9, 0, 2, 5, 7, 2, 5, 4, 9])\n",
      "Reminder: target label is : \n",
      "tensor(3)\n",
      "\n",
      " \n",
      " The counterfactual_id is :: \n",
      "[130]\n",
      "\n",
      " \n",
      " This is the path of counterfactuals: \n",
      "[130.]\n"
     ]
    }
   ],
   "source": [
    "def load_mnist(\n",
    "    batch_size: int, train: bool, subset_size=None, shuffle: bool = True\n",
    ") -> DataLoader:\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        \"./data/\",\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    if subset_size:\n",
    "        dataset = torch.utils.data.Subset(\n",
    "            dataset, torch.randperm(len(dataset))[:subset_size]\n",
    "        )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "# Load data:\n",
    "corpus_size = 500\n",
    "batch_size_test = 7\n",
    "test_id = 3\n",
    "\n",
    "corpus_loader = load_mnist(corpus_size, train=True)\n",
    "corpus_examples = enumerate(corpus_loader)\n",
    "_, (corpus_inputs, corpus_target) = next(corpus_examples)\n",
    "corpus_inputs = corpus_inputs.detach()\n",
    "\n",
    "test_loader = load_mnist(batch_size_test, train=False)\n",
    "test_examples = enumerate(test_loader)\n",
    "_, (test_inputs, test_target) = next(test_examples)\n",
    "\n",
    "new_tensor = test_inputs\n",
    "# print(test_target.shape)\n",
    "# print(test_target)\n",
    "# print(test_inputs.shape)\n",
    "# print(test_inputs)\n",
    "\n",
    "print(4*\"\\n\")\n",
    "\n",
    "#changed\n",
    "test_inputs = test_inputs[test_id:test_id+1].detach()\n",
    "test_inputs = test_inputs[0]\n",
    "test_input_label = test_target[test_id:test_id+1]\n",
    "\n",
    "# print(test_input_label.shape)\n",
    "# print(test_input_label)\n",
    "# print(test_inputs.shape)\n",
    "# print(test_inputs)\n",
    "\n",
    "\n",
    "# print(5* \"\\n\")\n",
    "\n",
    "##### COUNTERFACTUAL IDXs\n",
    "N = corpus_size\n",
    "counterfacts_onehot = np.zeros((N))\n",
    "for i in range(N):\n",
    "    if corpus_target[i] != test_input_label:\n",
    "        counterfacts_onehot[i] = 1\n",
    "counterfacts_idx = np.where(counterfacts_onehot==1)[0]\n",
    "counterfacts = corpus_inputs[counterfacts_idx]\n",
    "N_cf = int(np.sum(counterfacts_onehot))\n",
    "\n",
    "if N_cf== 0: \n",
    "    print(\"   WARNING: NO COUNTERFACTUAL FOUND\")\n",
    "    quit()\n",
    "\n",
    "print(\"target label (test point label) :  \")\n",
    "print(test_input_label[0])\n",
    "print(\"corpus labels :  \")\n",
    "print(corpus_target)\n",
    "print(\"cf potentials :\")\n",
    "print(counterfacts_onehot) \n",
    "print(\"CF indices\")\n",
    "print(counterfacts_idx)\n",
    "print(\"\\n \\n Now we print the second counterfactual's index \\n \")\n",
    "print(counterfacts_idx[1])\n",
    "print(\"CFs labels\")\n",
    "print(corpus_inputs[counterfacts_idx].shape)\n",
    "print(\"number cfs\")\n",
    "print(N_cf)\n",
    "print(3*\"\\n\")\n",
    "\n",
    "print(\"test_inputs: of dimension  \")\n",
    "print(test_inputs.shape)\n",
    "print(\"flattened: \")\n",
    "print(torch.flatten(test_inputs).shape)\n",
    "\n",
    "\n",
    "##### COUNTERFACTUAL Distances\n",
    "x_test = torch.flatten(test_inputs)\n",
    "dist_cf = np.zeros((N_cf))\n",
    "for i in range(N_cf):\n",
    "    cf_idx = counterfacts_idx[i]\n",
    "    x_cf = torch.flatten(corpus_inputs[cf_idx])\n",
    "    print(corpus_inputs[cf_idx].shape)\n",
    "    print(torch.flatten(corpus_inputs[cf_idx]).shape)\n",
    "    dist = np.linalg.norm(x_test - x_cf, 2)\n",
    "    dist_cf[i] = dist\n",
    "\n",
    "print(2*\"\\n\")\n",
    "print(\"Here are the counterfactual distances to test_point\")\n",
    "print(dist_cf )\n",
    "print(2*\"\\n\")\n",
    "\n",
    "  \n",
    "    \n",
    "##### CALCULATE MASSIVE ADJACENCY MATRIX\n",
    "# The matrix is composed of the corpus and the test point.\n",
    "adj = np.zeros((N+1,N+1))\n",
    "for i in range(N+1):\n",
    "    if i<N:\n",
    "        xi = torch.flatten(corpus_inputs[i])\n",
    "    else:\n",
    "        xi = torch.flatten(test_inputs)\n",
    "        \n",
    "    for j in range(N+1): \n",
    "        if j<N:\n",
    "            xj = torch.flatten(corpus_inputs[j])\n",
    "        else:\n",
    "            xj = torch.flatten(test_inputs)\n",
    " \n",
    "        dist = np.linalg.norm(xi - xj, 2)\n",
    "        adj[i,j] = dist\n",
    "        adj[j,i] = dist\n",
    " \n",
    "print(\"Here is the adjacency matrix\")\n",
    "print(adj.shape)\n",
    "print(adj)\n",
    "\n",
    "# Get rid of the 40% longest edges\n",
    "distances = np.zeros((N+1))\n",
    "k = int(np.floor(0.2*N))\n",
    "new_adj = np.zeros((N+1, N+1))\n",
    "\n",
    "for i in range(N+1): \n",
    "    distances = adj[i]\n",
    "    \n",
    "    # find k smallest elements of adj[i,:] \n",
    "    idx = np.argpartition(distances, k)\n",
    "    \n",
    "    # find largest values of those k elements\n",
    "    # max_dist_k = np.amax(distances[idx])\n",
    "    max_dist_k = np.percentile(distances, 70)\n",
    "    \n",
    "    # discard distances larger than max_dist_k\n",
    "    distances[distances>=max_dist_k] = 0\n",
    "\n",
    "    # update the new adjacency matrix \n",
    "    new_adj[i] = distances\n",
    "    \n",
    "print(\"\\n We have a new adjacency matrix: \")\n",
    "print(new_adj)\n",
    "\n",
    "\n",
    "\n",
    "##### CALCULATE SHORTEST PATHS\n",
    "csgraph = csr_matrix(new_adj)\n",
    "shortest_paths, predecessors = shortest_path(csgraph, method='auto', directed=True, return_predecessors=True, unweighted=False, overwrite=False, indices=None)\n",
    "\n",
    "print(\"\\n Shortest paths matrix: \")\n",
    "print(shortest_paths)\n",
    "\n",
    "print(\"\\n Predecessors matrix: \")\n",
    "print(predecessors)\n",
    "\n",
    "\n",
    "\n",
    "# Find counterfactual with shortest path \n",
    "counterfacts_distances = shortest_paths[N][counterfacts_idx]\n",
    "shortest_cft_dist = np.amin(counterfacts_distances)\n",
    "shortest_cft_id = np.where(new_adj[N]==shortest_cft_dist)[0]\n",
    "\n",
    "\n",
    "print(\"\\n Shortest distances between counterfactuals and test point:\")\n",
    "print(counterfacts_distances)\n",
    "print(\"Reminder: counterfactual indices are: \")\n",
    "print(counterfacts_idx)\n",
    "print(\"Reminder: counterfactual labels are: \")\n",
    "print(corpus_target[counterfacts_idx])\n",
    "print(\"Reminder: target label is : \")\n",
    "print(test_input_label[0])\n",
    "\n",
    "print(\"\\n \\n The counterfactual_id is :: \")\n",
    "print(shortest_cft_id)\n",
    "\n",
    "\n",
    "# Extract path from test point to counterfactual  \n",
    "current_idx = shortest_cft_id\n",
    "path_indices = np.zeros((1))\n",
    "path_indices[0] = shortest_cft_id\n",
    "while predecessors[N][current_idx] >= 0:\n",
    "    current_idx = predecessors[N][current_idx]\n",
    "    np.append(path_indices, current_idx)\n",
    "print(\"\\n \\n This is the path of counterfactuals: \")\n",
    "print(path_indices)\n",
    "    \n",
    "    \n",
    "\n",
    "# Extract intermediary example from this path \n",
    "if len(path_indices) > 1: \n",
    "    if len(path_indices) == 2:\n",
    "        intermediary_id = path_indices[1]\n",
    "    else:\n",
    "        intermediary_id = path_indices[len(path_indices//2)]\n",
    "\n",
    "# PERFORM SIMPLEX ANALYSIS ON INTERMEDIARY AND COUNTERFACTUAL  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "# within mnist find how they transfer it to Simplex\n",
    "# manipulate the corpus\n",
    "\n",
    "def load_mnist(\n",
    "    batch_size: int, train: bool, subset_size=None, shuffle: bool = True\n",
    ") -> DataLoader:\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        \"./data/\",\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    if subset_size:\n",
    "        dataset = torch.utils.data.Subset(\n",
    "            dataset, torch.randperm(len(dataset))[:subset_size]\n",
    "        )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "# Load data:\n",
    "corpus_size = 20\n",
    "test_size = 5\n",
    "device=torch.device\n",
    "\n",
    "corpus_loader = load_mnist(corpus_size, train=True)\n",
    "corpus_examples = enumerate(corpus_loader)\n",
    "batch_id_corpus, (corpus_data, corpus_target) = next(corpus_examples)\n",
    "corpus_data = corpus_data.detach()\n",
    "\n",
    "print(\"\\n corpus downloaded: \\n\")\n",
    "print(corpus_data.shape)\n",
    "print(\"corpus first elem: \")\n",
    "print(corpus_data[0].shape)\n",
    "print(\"\\n corpus target \\n\")\n",
    "print(corpus_target)\n",
    "\n",
    "##### CALCULATE ADJACENCY MATRIX\n",
    "N = corpus_size\n",
    "print(\"the corpus size\")\n",
    "print(N)\n",
    "print(corpus_data.shape[0])\n",
    "adj = np.zeros((N,N))\n",
    "avg_distances = np.zeros((N))\n",
    "for i in range(N):\n",
    "    avg_dist_i = 0\n",
    "    for j in range(0, N):        \n",
    "        xi = torch.flatten(corpus_data[i])\n",
    "        xj = torch.flatten(corpus_data[j])\n",
    "        dist = np.linalg.norm(xi - xj, 2)\n",
    "        adj[i, j] = dist\n",
    "        avg_dist_i = avg_dist_i + dist\n",
    "    avg_distances[i] = avg_dist_i / (N-1)\n",
    "\n",
    "print(\"\\n Here is the adjacency list \\n\")\n",
    "print(adj.shape)\n",
    "print(adj)\n",
    "print(\"\\n Avg distances \\n\")\n",
    "print(avg_distances)\n",
    "    \n",
    "    \n",
    "##### CALCULATE AVG DIST to K nearest neighbour\n",
    "k = N // 6\n",
    "for i in range(N):\n",
    "    avg_dist_i = 0\n",
    "    A = adj[i]\n",
    "    idx = np.argpartition(A, k)\n",
    "    avg_dist_i = np.mean(A[idx[:k]])\n",
    "    avg_distances[i] = avg_dist_i \n",
    "\n",
    "print(\"\\n Avg dist from 2 nearest neighbours \\n\")\n",
    "print(avg_distances)\n",
    "\n",
    "##### KEEP CLOSEST 80th PERCENTILE for new corpus\n",
    "N_keep = int(np.ceil(0.6*N))\n",
    "idx = np.argpartition(avg_distances, N_keep)\n",
    "\n",
    "new_corpus_data = corpus_data[idx[:N_keep]]\n",
    "print(\"\\n OG Corpus \\n\")\n",
    "print(corpus_data.shape)\n",
    "print(\"\\n new corpus Corpus \\n\")\n",
    "print(new_corpus_data.shape)\n",
    "\n",
    "\n",
    "for idx, elem in enumerate(corpus_data):\n",
    "    \n",
    "    if idx==3:\n",
    "        print(\"\\n \\n we are taking the fourth corpus elem \\n \")\n",
    "        print(elem.shape)\n",
    "        print(elem[0].shape)\n",
    "        print(elem[0][0].shape)\n",
    "        print(elem[0][0][0].shape)\n",
    "        print(\"\\n       new corpus elem \\n \")\n",
    "        new_elem = torch.flatten(elem)\n",
    "        print(new_elem.shape)\n",
    "        print(new_elem)\n",
    "        \n",
    "        \n",
    "## Construct the Corpus: \n",
    "# construct the corpus\n",
    "# construct the #size of the corpus\n",
    "\n",
    "\n",
    "        \n",
    "\"\"\"\n",
    "print(\"\\n Changing corpus to numpy\")\n",
    "X = corpus_examples.data.numpy()\n",
    "print(X)\n",
    "N = len(X)\n",
    "mask = np.ones(N, dtype=bool)\n",
    "# kernel density estimator\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=3).fit(X)\n",
    "log_density = kde.score_samples(X)\n",
    "uniform_log_density = np.log(1/N)\n",
    "for i in range(0,N):\n",
    "    if log_density[i] > uniform_log_density:\n",
    "        # add xi to corpus\n",
    "        mask[i] = False \n",
    "X = X[mask]\n",
    "\n",
    "self.corpus_examples =  torch.from_numpy(X)\n",
    "print(\"\\n Corpus selected and updated\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "  # def construct_corpus(\n",
    "    #     self,\n",
    "    #     eps_dist: float = 1.0,\n",
    "    # ) -> None: \n",
    "    #     \"\"\"\n",
    "    #     Constructs and modifies corpus, \n",
    "    #     :param eps_dist: \n",
    "    #     \"\"\"\n",
    "    #     print(\"\\n We are now selectin corpus\")\n",
    "    #     X = self.corpus_examples.data.numpy()\n",
    "    #     N = len(X)\n",
    "    #     mask = np.ones(N, dtype=bool)\n",
    "    #     # kernel density estimator\n",
    "    #     kde = KernelDensity(kernel='gaussian', bandwidth=3).fit(X)\n",
    "    #     log_density = kde.score_samples(X)\n",
    "    #     uniform_log_density = np.log(1/N)\n",
    "    #     for i in range(0,N):\n",
    "    #         if log_density[i] > uniform_log_density:\n",
    "    #             # add xi to corpus\n",
    "    #             mask[i] = False \n",
    "    #     X = X[mask]\n",
    "\n",
    "        # self.corpus_examples =  torch.from_numpy(X)\n",
    "        # print(\"\\n Corpus selected and updated\")\n",
    "\n",
    "\n",
    "\"\"\"    \n",
    "    \n",
    "##### CALCULATE AVG DIST to K nearest neighbour\n",
    "k = N // 6\n",
    "for i in range(N):\n",
    "    avg_dist_i = 0\n",
    "    A = adj[i]\n",
    "    idx = np.argpartition(A, k)\n",
    "    avg_dist_i = np.mean(A[idx[:k]])\n",
    "    avg_distances[i] = avg_dist_i \n",
    "\n",
    "print(\"\\n Avg dist from 2 nearest neighbours \\n\")\n",
    "print(avg_distances)\n",
    "\n",
    "##### KEEP CLOSEST 80th PERCENTILE for new corpus\n",
    "N_keep = int(np.ceil(0.6*N))\n",
    "idx = np.argpartition(avg_distances, N_keep)\n",
    "\n",
    "new_corpus_data = corpus_data[idx[:N_keep]]\n",
    "print(\"\\n OG Corpus \\n\")\n",
    "print(corpus_data.shape)\n",
    "print(\"\\n new corpus Corpus \\n\")\n",
    "print(new_corpus_data.shape)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "batch_id_test, (test_data, test_targets) = next(test_examples)\n",
    "batch_id_corpus, (corpus_data, corpus_target) = next(corpus_examples)\n",
    "corpus_data = corpus_data.to(device).detach()\n",
    "test_data = test_data.to(device).detach()\n",
    "corpus_latent_reps = classifier.latent_representation(corpus_data).detach()\n",
    "corpus_probas = classifier.probabilities(corpus_data).detach()\n",
    "corpus_true_classes = torch.zeros(corpus_probas.shape, device=device)\n",
    "corpus_true_classes[torch.arange(corpus_size), corpus_target] = 1\n",
    "test_latent_reps = classifier.latent_representation(test_data).detach()\n",
    "\n",
    "\n",
    "def fit_explainers(\n",
    "    device: torch.device,\n",
    "    explainers_name: list,\n",
    "    save_path: Path,\n",
    "    corpus_size: int = 1000,\n",
    "    test_size: int = 100,\n",
    "    n_epoch: int = 10000,\n",
    "    random_seed: int = 42,\n",
    "    n_keep: int = 5,\n",
    "    reg_factor_init: float = 0.1,\n",
    "    reg_factor_final: float = 100,\n",
    "    cv: int = 0,\n",
    "    train_only: bool = False,\n",
    ") -> list:\n",
    "    torch.random.manual_seed(random_seed + cv)\n",
    "    explainers = []\n",
    "\n",
    "    # Load model:\n",
    "    classifier = MnistClassifier()\n",
    "    classifier.load_state_dict(torch.load(save_path / f\"model_cv{cv}.pth\"))\n",
    "    classifier.to(device)\n",
    "    classifier.eval()\n",
    "\n",
    "    \n",
    "\n",
    "    # Fit ContEx:\n",
    "    reg_factor_scheduler = ExponentialScheduler(\n",
    "        reg_factor_init, reg_factor_final, n_epoch\n",
    "    )\n",
    "    contex = Contex(\n",
    "        corpus_examples=corpus_data, corpus_latent_reps=corpus_latent_reps\n",
    "    )\n",
    "    contex.construct_corpus(\n",
    "        eps_dist=1.0,\n",
    "    )\n",
    "    contex.fit(\n",
    "        test_examples=test_data,\n",
    "        test_latent_reps=test_latent_reps,\n",
    "        n_epoch=n_epoch,\n",
    "        reg_factor=reg_factor_init,\n",
    "        n_keep=n_keep,\n",
    "        reg_factor_scheduler=reg_factor_scheduler,\n",
    "    )\n",
    "    explainers.append(contex)\n",
    "\n",
    "    # Fit SimplEx:\n",
    "    reg_factor_scheduler = ExponentialScheduler(\n",
    "        reg_factor_init, reg_factor_final, n_epoch\n",
    "    )\n",
    "    simplex = Simplex(\n",
    "        corpus_examples=corpus_data, corpus_latent_reps=corpus_latent_reps\n",
    "    )\n",
    "    simplex.fit(\n",
    "        test_examples=test_data,\n",
    "        test_latent_reps=test_latent_reps,\n",
    "        n_epoch=n_epoch,\n",
    "        reg_factor=reg_factor_init,\n",
    "        n_keep=n_keep,\n",
    "        reg_factor_scheduler=reg_factor_scheduler,\n",
    "    )\n",
    "    explainers.append(simplex)\n",
    "\n",
    "    \"\"\"\n",
    "    # Fit nearest neighbors:\n",
    "    nn_uniform = NearNeighLatent(\n",
    "        corpus_examples=corpus_data, corpus_latent_reps=corpus_latent_reps\n",
    "    )\n",
    "    nn_uniform.fit(\n",
    "        test_examples=test_data, test_latent_reps=test_latent_reps, n_keep=n_keep\n",
    "    )\n",
    "    explainers.append(nn_uniform)\n",
    "    nn_dist = NearNeighLatent(\n",
    "        corpus_examples=corpus_data,\n",
    "        corpus_latent_reps=corpus_latent_reps,\n",
    "        weights_type=\"distance\",\n",
    "    )\n",
    "    nn_dist.fit(\n",
    "        test_examples=test_data, test_latent_reps=test_latent_reps, n_keep=n_keep\n",
    "    )\n",
    "    explainers.append(nn_dist)\n",
    "    \"\"\"\n",
    "\n",
    "    # Save explainers and data:\n",
    "    for explainer, explainer_name in zip(explainers, explainers_name):\n",
    "        explainer_path = save_path / f\"{explainer_name}_cv{cv}_n{n_keep}.pkl\"\n",
    "        with open(explainer_path, \"wb\") as f:\n",
    "            print(f\"Saving {explainer_name} decomposition in {explainer_path}.\")\n",
    "            pkl.dump(explainer, f)\n",
    "    corpus_data_path = save_path / f\"corpus_data_cv{cv}.pkl\"\n",
    "    with open(corpus_data_path, \"wb\") as f:\n",
    "        print(f\"Saving corpus data in {corpus_data_path}.\")\n",
    "        pkl.dump([corpus_latent_reps, corpus_probas, corpus_true_classes], f)\n",
    "    test_data_path = save_path / f\"test_data_cv{cv}.pkl\"\n",
    "    with open(test_data_path, \"wb\") as f:\n",
    "        print(f\"Saving test data in {test_data_path}.\")\n",
    "        pkl.dump([test_latent_reps, test_targets], f)\n",
    "    return explainers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def approximation_quality(\n",
    "    n_keep_list: list,\n",
    "    cv: int = 0,\n",
    "    random_seed: int = 42,\n",
    "    model_reg_factor=0.1,\n",
    "    save_path: str = \"results/mnist/quality/\",\n",
    "    train_only=False,\n",
    ") -> None:\n",
    "    print(\n",
    "        100 * \"-\"\n",
    "        + \"\\n\"\n",
    "        + \"Welcome in the approximation quality experiment for MNIST. \\n\"\n",
    "        f\"Settings: random_seed = {random_seed} ; cv = {cv}.\\n\" + 100 * \"-\"\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #explainers_name = [\"simplex\", \"nn_uniform\", \"nn_dist\", \"representer\"]\n",
    "    explainers_name = [\"contex\", \"simplex\"]\n",
    "    print(explainers_name)\n",
    "    current_path = Path.cwd()\n",
    "    save_path = current_path / save_path\n",
    "    # Create saving directory if inexistent\n",
    "    if not save_path.exists():\n",
    "        print(f\"Creating the saving directory {save_path}\")\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Training a model, save it\n",
    "    print(100 * \"-\" + \"\\n\" + \"Now fitting the model. \\n\" + 100 * \"-\")\n",
    "    train_model(\n",
    "        device=device,\n",
    "        random_seed=random_seed,\n",
    "        cv=cv,\n",
    "        save_path=save_path,\n",
    "        model_reg_factor=model_reg_factor,\n",
    "    )\n",
    "\n",
    "    # Load the model\n",
    "    classifier = MnistClassifier()\n",
    "    classifier.load_state_dict(torch.load(save_path / f\"model_cv{cv}.pth\"))\n",
    "    classifier.to(device)\n",
    "    classifier.eval()\n",
    "\n",
    "    # Fit the explainers\n",
    "    print(100 * \"-\" + \"\\n\" + \"Now fitting the explainers. \\n\" + 100 * \"-\")\n",
    "    for i, n_keep in enumerate(n_keep_list):\n",
    "        print(30 * \"-\" + f\"n_keep = {n_keep}\" + 30 * \"-\")\n",
    "        explainers = fit_explainers(\n",
    "            device=device,\n",
    "            random_seed=random_seed,\n",
    "            cv=cv,\n",
    "            test_size=100,\n",
    "            corpus_size=1000,\n",
    "            n_keep=n_keep,\n",
    "            save_path=save_path,\n",
    "            explainers_name=explainers_name,\n",
    "            train_only=train_only,\n",
    "        )\n",
    "        # Print the partial results\n",
    "        print(100 * \"-\" + \"\\n\" + \"Results. \\n\" + 100 * \"-\")\n",
    "        #for explainer, explainer_name in zip(explainers, explainers_name[:-1]):\n",
    "        for explainer, explainer_name in zip(explainers, explainers_name):\n",
    "            latent_rep_approx = explainer.latent_approx()\n",
    "            latent_rep_true = explainer.test_latent_reps\n",
    "            output_approx = classifier.latent_to_presoftmax(latent_rep_approx).detach()\n",
    "            output_true = classifier.latent_to_presoftmax(latent_rep_true).detach()\n",
    "            latent_r2_score = sklearn.metrics.r2_score(\n",
    "                latent_rep_true.cpu().numpy(), latent_rep_approx.cpu().numpy()\n",
    "            )\n",
    "            output_r2_score = sklearn.metrics.r2_score(\n",
    "                output_true.cpu().numpy(), output_approx.cpu().numpy()\n",
    "            )\n",
    "            print(\n",
    "                f\"{explainer_name} latent r2: {latent_r2_score:.2g} ; output r2 = {output_r2_score:.2g}.\"\n",
    "            )\n",
    "\n",
    "    \"\"\"\n",
    "    # Fit the representer explainer (this is only makes sense by using the whole corpus)\n",
    "    representer = fit_representer(model_reg_factor, save_path, cv)\n",
    "    latent_rep_true = representer.test_latent_reps\n",
    "    output_true = classifier.latent_to_presoftmax(latent_rep_true).detach()\n",
    "    output_approx = representer.output_approx()\n",
    "    output_r2_score = sklearn.metrics.r2_score(\n",
    "        output_true.cpu().numpy(), output_approx.cpu().numpy()\n",
    "    )\n",
    "    print(f\"representer output r2 = {output_r2_score:.2g}.\")\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def construct_corpus(\n",
    "        self,\n",
    "        eps_dist: float = 1.0,\n",
    "    ) -> None: \n",
    "        \"\"\"\n",
    "        Constructs and modifies corpus, \n",
    "        :param eps_dist: \n",
    "        \"\"\"\n",
    "        print(\"\\n We are now selectin corpus\")\n",
    "        X = self.corpus_examples.data.numpy()\n",
    "        N = len(X)\n",
    "        mask = np.ones(N, dtype=bool)\n",
    "        # kernel density estimator\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=3).fit(X)\n",
    "        log_density = kde.score_samples(X)\n",
    "        uniform_log_density = np.log(1/N)\n",
    "        for i in range(0,N):\n",
    "            if log_density[i] > uniform_log_density:\n",
    "                # add xi to corpus\n",
    "                mask[i] = False \n",
    "        X = X[mask]\n",
    "\n",
    "        self.corpus_examples =  torch.from_numpy(X)\n",
    "        print(\"\\n Corpus selected and updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the prostate cancer time series data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(random_seed=42, corpus_size=100, batch_size=50):\n",
    "    # LOAD DATA from file\n",
    "    (\n",
    "        X,\n",
    "        y,\n",
    "        feature_names,\n",
    "        max_time_points,\n",
    "        rescale_dict,\n",
    "    ) = load_time_series_prostate_cancer()\n",
    "\n",
    "    # Get data into shape and produce corpus\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=random_seed, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Train set contains {len(y_train)} records\")\n",
    "    print(f\"Test set contains {len(y_test)} records\")\n",
    "    print(f\"{sum(y_train == 1)} training records with a label of 1\")\n",
    "    print(f\"{sum(y_train == 0)} training records with a label of 0\")\n",
    "    print(f\"{sum(y_test == 1)} test records with a label of 1\")\n",
    "    print(f\"{sum(y_test == 0)} test records with a label of 0\")\n",
    "\n",
    "    class_imbalance_weighting = sum(y_train == 0) / len(y_train)\n",
    "\n",
    "    train_data = TimeSeriesProstateCancerDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_data = TimeSeriesProstateCancerDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    test_examples = enumerate(test_loader)\n",
    "    batch_id_test, (test_inputs, test_targets) = next(test_examples)\n",
    "\n",
    "    corpus_loader = DataLoader(train_data, batch_size=corpus_size, shuffle=False)\n",
    "    corpus_examples = enumerate(corpus_loader)\n",
    "    batch_id_corpus, (corpus_inputs, corpus_targets) = next(corpus_examples)\n",
    "\n",
    "    input_baseline = torch.mean(torch.mean(corpus_inputs, 1), 0).expand(\n",
    "        100, max_time_points, -1\n",
    "    )  # Baseline tensor of the same shape as corpus_inputs\n",
    "\n",
    "    return (\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        corpus_inputs,\n",
    "        corpus_targets,\n",
    "        test_inputs,\n",
    "        test_targets,\n",
    "        max_time_points,\n",
    "        feature_names,\n",
    "        class_imbalance_weighting,\n",
    "        input_baseline,\n",
    "        rescale_dict,\n",
    "    )\n",
    "\n",
    "\n",
    "# LOAD data\n",
    "batch_size = 50\n",
    "corpus_size = 100\n",
    "\n",
    "(\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    corpus_inputs,\n",
    "    corpus_targets,\n",
    "    test_inputs,\n",
    "    test_targets,\n",
    "    max_time_points,\n",
    "    feature_names,\n",
    "    class_imbalance_weighting,\n",
    "    input_baseline,\n",
    "    rescale_dict,\n",
    ") = load_data(random_seed=5, corpus_size=corpus_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "# within mnist find how they transfer it to Simplex\n",
    "# manipulate the corpus\n",
    "\n",
    "\n",
    "def load_mnist(\n",
    "    batch_size: int, train: bool, subset_size=None, shuffle: bool = True\n",
    ") -> DataLoader:\n",
    "    dataset = torchvision.datasets.MNIST(\n",
    "        \"./data/\",\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    if subset_size:\n",
    "        dataset = torch.utils.data.Subset(\n",
    "            dataset, torch.randperm(len(dataset))[:subset_size]\n",
    "        )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "\n",
    "def fit_explainers(\n",
    "    device: torch.device,\n",
    "    explainers_name: list,\n",
    "    save_path: Path,\n",
    "    corpus_size: int = 1000,\n",
    "    test_size: int = 100,\n",
    "    n_epoch: int = 10000,\n",
    "    random_seed: int = 42,\n",
    "    n_keep: int = 5,\n",
    "    reg_factor_init: float = 0.1,\n",
    "    reg_factor_final: float = 100,\n",
    "    cv: int = 0,\n",
    "    train_only: bool = False,\n",
    ") -> list:\n",
    "    torch.random.manual_seed(random_seed + cv)\n",
    "    explainers = []\n",
    "\n",
    "    # Load model:\n",
    "    classifier = MnistClassifier()\n",
    "    classifier.load_state_dict(torch.load(save_path / f\"model_cv{cv}.pth\"))\n",
    "    classifier.to(device)\n",
    "    classifier.eval()\n",
    "\n",
    "    # Load data:\n",
    "    corpus_loader = load_mnist(corpus_size, train=True)\n",
    "    test_loader = load_mnist(test_size, train=train_only)\n",
    "    corpus_examples = enumerate(corpus_loader)\n",
    "    test_examples = enumerate(test_loader)\n",
    "    batch_id_test, (test_data, test_targets) = next(test_examples)\n",
    "    batch_id_corpus, (corpus_data, corpus_target) = next(corpus_examples)\n",
    "    corpus_data = corpus_data.to(device).detach()\n",
    "    test_data = test_data.to(device).detach()\n",
    "    corpus_latent_reps = classifier.latent_representation(corpus_data).detach()\n",
    "    corpus_probas = classifier.probabilities(corpus_data).detach()\n",
    "    corpus_true_classes = torch.zeros(corpus_probas.shape, device=device)\n",
    "    corpus_true_classes[torch.arange(corpus_size), corpus_target] = 1\n",
    "    test_latent_reps = classifier.latent_representation(test_data).detach()\n",
    "\n",
    "    # Fit ContEx:\n",
    "    reg_factor_scheduler = ExponentialScheduler(\n",
    "        reg_factor_init, reg_factor_final, n_epoch\n",
    "    )\n",
    "    contex = Contex(\n",
    "        corpus_examples=corpus_data, corpus_latent_reps=corpus_latent_reps\n",
    "    )\n",
    "    contex.construct_corpus(\n",
    "        eps_dist=1.0,\n",
    "    )\n",
    "    contex.fit(\n",
    "        test_examples=test_data,\n",
    "        test_latent_reps=test_latent_reps,\n",
    "        n_epoch=n_epoch,\n",
    "        reg_factor=reg_factor_init,\n",
    "        n_keep=n_keep,\n",
    "        reg_factor_scheduler=reg_factor_scheduler,\n",
    "    )\n",
    "    explainers.append(contex)\n",
    "\n",
    "    # Fit SimplEx:\n",
    "    reg_factor_scheduler = ExponentialScheduler(\n",
    "        reg_factor_init, reg_factor_final, n_epoch\n",
    "    )\n",
    "    simplex = Simplex(\n",
    "        corpus_examples=corpus_data, corpus_latent_reps=corpus_latent_reps\n",
    "    )\n",
    "    simplex.fit(\n",
    "        test_examples=test_data,\n",
    "        test_latent_reps=test_latent_reps,\n",
    "        n_epoch=n_epoch,\n",
    "        reg_factor=reg_factor_init,\n",
    "        n_keep=n_keep,\n",
    "        reg_factor_scheduler=reg_factor_scheduler,\n",
    "    )\n",
    "    explainers.append(simplex)\n",
    "\n",
    "    \"\"\"\n",
    "    # Fit nearest neighbors:\n",
    "    nn_uniform = NearNeighLatent(\n",
    "        corpus_examples=corpus_data, corpus_latent_reps=corpus_latent_reps\n",
    "    )\n",
    "    nn_uniform.fit(\n",
    "        test_examples=test_data, test_latent_reps=test_latent_reps, n_keep=n_keep\n",
    "    )\n",
    "    explainers.append(nn_uniform)\n",
    "    nn_dist = NearNeighLatent(\n",
    "        corpus_examples=corpus_data,\n",
    "        corpus_latent_reps=corpus_latent_reps,\n",
    "        weights_type=\"distance\",\n",
    "    )\n",
    "    nn_dist.fit(\n",
    "        test_examples=test_data, test_latent_reps=test_latent_reps, n_keep=n_keep\n",
    "    )\n",
    "    explainers.append(nn_dist)\n",
    "    \"\"\"\n",
    "\n",
    "    # Save explainers and data:\n",
    "    for explainer, explainer_name in zip(explainers, explainers_name):\n",
    "        explainer_path = save_path / f\"{explainer_name}_cv{cv}_n{n_keep}.pkl\"\n",
    "        with open(explainer_path, \"wb\") as f:\n",
    "            print(f\"Saving {explainer_name} decomposition in {explainer_path}.\")\n",
    "            pkl.dump(explainer, f)\n",
    "    corpus_data_path = save_path / f\"corpus_data_cv{cv}.pkl\"\n",
    "    with open(corpus_data_path, \"wb\") as f:\n",
    "        print(f\"Saving corpus data in {corpus_data_path}.\")\n",
    "        pkl.dump([corpus_latent_reps, corpus_probas, corpus_true_classes], f)\n",
    "    test_data_path = save_path / f\"test_data_cv{cv}.pkl\"\n",
    "    with open(test_data_path, \"wb\") as f:\n",
    "        print(f\"Saving test data in {test_data_path}.\")\n",
    "        pkl.dump([test_latent_reps, test_targets], f)\n",
    "    return explainers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def construct_corpus(\n",
    "        self,\n",
    "        eps_dist: float = 1.0,\n",
    "    ) -> None: \n",
    "        \"\"\"\n",
    "        Constructs and modifies corpus, \n",
    "        :param eps_dist: \n",
    "        \"\"\"\n",
    "        print(\"\\n We are now selectin corpus\")\n",
    "        X = self.corpus_examples.data.numpy()\n",
    "        N = len(X)\n",
    "        mask = np.ones(N, dtype=bool)\n",
    "        # kernel density estimator\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=3).fit(X)\n",
    "        log_density = kde.score_samples(X)\n",
    "        uniform_log_density = np.log(1/N)\n",
    "        for i in range(0,N):\n",
    "            if log_density[i] > uniform_log_density:\n",
    "                # add xi to corpus\n",
    "                mask[i] = False \n",
    "        X = X[mask]\n",
    "\n",
    "        self.corpus_examples =  torch.from_numpy(X)\n",
    "        print(\"\\n Corpus selected and updated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get trained mortality model\n",
    "\n",
    "Use the boolean variable `train_model` to either train a new model or load a pre-trained model from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model, trained_model_state_path):\n",
    "    model.load_state_dict(torch.load(trained_model_state_path))\n",
    "    # model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get a trained model\n",
    "save_path = os.path.abspath(\n",
    "    f\"../demonstrator/resources/trained_models/RNN/Time series Prostate Cancer/\"\n",
    ")\n",
    "cv = 1\n",
    "# Train model if required\n",
    "train_model = False\n",
    "\n",
    "if train_model:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Define parameters\n",
    "    n_epoch_model = 600\n",
    "    log_interval = 3\n",
    "    weight_decay = 1e-6\n",
    "\n",
    "    # Create the model\n",
    "    classifier = MortalityGRU(\n",
    "        input_dim=len(feature_names),\n",
    "        hidden_dim=5,\n",
    "        output_dim=1,\n",
    "        n_layers=1,\n",
    "    )\n",
    "    print(classifier)\n",
    "    classifier.to(device)\n",
    "    class_weights = torch.FloatTensor([class_imbalance_weighting]).to(device)\n",
    "    criterion = nn.BCELoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(classifier.parameters(), weight_decay=weight_decay)\n",
    "\n",
    "    # Train the model\n",
    "    print(100 * \"-\" + \"\\n\" + \"Now fitting the model. \\n\" + 100 * \"-\")\n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    test_losses = []\n",
    "\n",
    "    def train(epoch):\n",
    "        correct = 0\n",
    "        correct_0 = 0\n",
    "        correct_1 = 0\n",
    "        num_targets_0 = 0\n",
    "        num_targets_1 = 0\n",
    "\n",
    "        classifier.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            output = classifier(data)\n",
    "            target = target.type(torch.LongTensor).unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, target.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_roc_score = roc_auc_score(\n",
    "                target.cpu().detach(), output.cpu().detach()\n",
    "            )\n",
    "            pred = output.round()\n",
    "            num_targets_0 += len((target[target == 0]))\n",
    "            num_targets_1 += len((target[target == 1]))\n",
    "            correct_0 += float((pred[target == 0] == target[target == 0]).sum())\n",
    "            correct_1 += float((pred[target == 1] == target[target == 1]).sum())\n",
    "            correct += float((pred == target).sum())\n",
    "            if batch_idx % log_interval == 0:\n",
    "                train_losses.append(loss.item())\n",
    "                train_counter.append(\n",
    "                    (batch_idx * 128) + ((epoch - 1) * len(train_loader.dataset))\n",
    "                )\n",
    "\n",
    "        print(\n",
    "            \"TRAINING:\\n\"\n",
    "            f\"correct: {correct}/{len(train_loader.dataset)} ({100. * correct / len(train_loader.dataset):.0f}%)\\n\"\n",
    "            f\"correct 0s: {correct_0}/{num_targets_0} ({100. * correct_0 / num_targets_0:.0f}%)\\n\"\n",
    "            f\"correct 1s: {correct_1}/{num_targets_1} ({100. * correct_1 / num_targets_1:.0f}%)\\n\"\n",
    "            f\"Training set: Avg. loss: {loss:.4f}, ROC AUC score: {train_roc_score:.4f}\"\n",
    "        )\n",
    "\n",
    "    def test():\n",
    "        classifier.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        correct_0 = 0\n",
    "        correct_1 = 0\n",
    "        num_targets_0 = 0\n",
    "        num_targets_1 = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "                data = data.to(device)\n",
    "                output = classifier(data)\n",
    "                target = target.type(torch.LongTensor).unsqueeze(1).to(device)\n",
    "                test_loss += criterion(output, target.float()).item()\n",
    "                pred = output.round()\n",
    "\n",
    "                test_roc_score = roc_auc_score(target.cpu(), output.cpu())\n",
    "                num_targets_0 += len((target[target == 0]))\n",
    "                num_targets_1 += len((target[target == 1]))\n",
    "                correct_0 += float((pred[target == 0] == target[target == 0]).sum())\n",
    "                correct_1 += float((pred[target == 1] == target[target == 1]).sum())\n",
    "                correct += float((pred == target).sum())\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        print(\n",
    "            \"TESTING:\\n\"\n",
    "            f\"correct: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f})%\\n\"\n",
    "            f\"correct 0s: {correct_0}/{num_targets_0} ({100. * correct_0 / num_targets_0:.0f})%\\n\"\n",
    "            f\"correct 1s: {correct_1}/{num_targets_1} ({100. * correct_1 / num_targets_1:.0f})%\\n\"\n",
    "            f\"Test set:     Avg. loss: {test_loss:.4f}, ROC AUC score: {test_roc_score:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"Pre-training test.\")\n",
    "    test()\n",
    "    print(\"\\n\\n\")\n",
    "    for epoch in range(1, n_epoch_model + 1):\n",
    "        print(f\"\\nepoch: {epoch}\")\n",
    "        train(epoch)\n",
    "        test()\n",
    "        torch.save(\n",
    "            classifier.state_dict(),\n",
    "            os.path.join(save_path, f\"model_cv{cv}.pth\"),\n",
    "        )\n",
    "        torch.save(\n",
    "            optimizer.state_dict(),\n",
    "            os.path.join(save_path, f\"optimizer_cv{cv}.pth\"),\n",
    "        )\n",
    "\n",
    "# Get a trained model\n",
    "model = MortalityGRU(\n",
    "    input_dim=len(feature_names),\n",
    "    hidden_dim=5,\n",
    "    output_dim=1,\n",
    "    n_layers=1,\n",
    ")  # Model should have the BlackBox interface\n",
    "TRAINED_MODEL_STATE_PATH = os.path.join(save_path, f\"model_cv{cv}.pth\")\n",
    "load_trained_model(model, TRAINED_MODEL_STATE_PATH)\n",
    "\n",
    "# Compute corpus and test model predictions\n",
    "corpus_predictions = model.forward(corpus_inputs).detach().round()\n",
    "test_predictions = model.forward(test_inputs).detach().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for sorting examples to match the sorted output from jacobian decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort order function for decomposition\n",
    "def apply_sort_order(in_list, sort_order):\n",
    "    if isinstance(in_list, list):\n",
    "        return [in_list[idx] for idx in sort_order]\n",
    "    if torch.is_tensor(in_list):\n",
    "        return [in_list.numpy()[idx] for idx in sort_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit SimplEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SimplEx\n",
    "# Compute the corpus and test latent representations\n",
    "corpus_latents = model.latent_representation(corpus_inputs).detach()\n",
    "test_latents = model.latent_representation(test_inputs).detach()\n",
    "# Initialize SimplEX, fit it on test examples\n",
    "simplex = Simplex(corpus_examples=corpus_inputs, corpus_latent_reps=corpus_latents)\n",
    "simplex.fit(\n",
    "    test_examples=test_inputs,\n",
    "    n_epoch=50000,\n",
    "    test_latent_reps=test_latents,\n",
    "    reg_factor=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the SimplEx decomposition for test patient i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Integrated Jacobian for a particular example\n",
    "i = 1\n",
    "simplex.jacobian_projection(test_id=i, model=model, input_baseline=input_baseline)\n",
    "result, sort_order = simplex.decompose(i, return_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Test Patient\n",
    "Use the variable `test_time_steps_to_display` to change the number of time steps is the displayed output. A value of 50 or greater will ensure all available time points are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataframes\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "test_time_steps_to_display = 3\n",
    "\n",
    "# Test patient\n",
    "test_patient_last_time_step_idx = (\n",
    "    simplex.test_examples[i][\n",
    "        ~np.all(simplex.test_examples[i].numpy() == 0, axis=1)\n",
    "    ].shape[0]\n",
    "    - 1\n",
    ")\n",
    "if (test_patient_last_time_step_idx + 1) - test_time_steps_to_display < 0:\n",
    "    test_time_steps_to_display = test_patient_last_time_step_idx + 1\n",
    "\n",
    "test_patient_df = pd.DataFrame(\n",
    "    simplex.test_examples[i][\n",
    "        test_patient_last_time_step_idx\n",
    "        - (test_time_steps_to_display - 1) : test_patient_last_time_step_idx\n",
    "        + 1,\n",
    "        :,\n",
    "    ].numpy(),\n",
    "    columns=feature_names,\n",
    "    index=[\n",
    "        f\"(t_max) - {i}\" if i != 0 else \"(t_max)\"\n",
    "        for i in reversed(range(test_time_steps_to_display))\n",
    "    ],\n",
    ")\n",
    "display(test_patient_df.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Corpus decomposition and importances\n",
    "\n",
    "Define styling functions for displaying dataframe.\n",
    "\n",
    "Define an `example_importance_threshold` to see all the members of the corpus that have an importance above that certain value and\n",
    "`corpus_time_steps_to_display` to set the number of time steps to include in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_values_to_colors(df):\n",
    "    \"\"\"Gets color values based in values relative to all other values in df.\"\"\"\n",
    "\n",
    "    min_val = np.nanmin(df.values)\n",
    "    max_val = np.nanmax(df.values)\n",
    "    for col in df:\n",
    "        # map values to colors in hex via\n",
    "        # creating a hex Look up table table and apply the normalized data to it\n",
    "        norm = mcolors.Normalize(\n",
    "            vmin=min_val,\n",
    "            vmax=max_val,\n",
    "            # vmin=np.nanmin(df[col].values),\n",
    "            # vmax=np.nanmax(df[col].values),\n",
    "            clip=True,\n",
    "        )\n",
    "        lut = plt.cm.bwr(np.linspace(0.2, 0.75, 256))\n",
    "        lut = np.apply_along_axis(mcolors.to_hex, 1, lut)\n",
    "        a = (norm(df[col].values) * 255).astype(np.int16)\n",
    "        df[col] = lut[a]\n",
    "    return df\n",
    "\n",
    "\n",
    "def highlight(x):\n",
    "    return pd.DataFrame(importance_df_colors.values, index=x.index, columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of patients\n",
    "\n",
    "# Variables\n",
    "example_importance_threshold = 0.1\n",
    "corpus_time_steps_to_display = 10\n",
    "\n",
    "\n",
    "# Patient Feature values\n",
    "last_time_step_idx = [\n",
    "    result[j][1][~np.all(result[j][1].numpy() == 0, axis=1)].shape[0] - 1\n",
    "    for j in range(len(result))\n",
    "]\n",
    "\n",
    "\n",
    "corpus_dfs = [\n",
    "    pd.DataFrame(\n",
    "        result[j][1][idx - (corpus_time_steps_to_display - 1) : idx + 1].numpy(),\n",
    "        columns=feature_names,\n",
    "    )\n",
    "    for j, idx in zip(range(len(result)), last_time_step_idx)\n",
    "]\n",
    "for corpus_df in corpus_dfs:\n",
    "    for col_name, rescale_value in rescale_dict.items():\n",
    "        corpus_df[col_name] = corpus_df[col_name].apply(lambda x: x * rescale_value)\n",
    "corpus_data = [\n",
    "    {\n",
    "        \"feature_vals\": corpus_dfs[i].transpose(),\n",
    "        \"Label\": apply_sort_order(corpus_targets, sort_order)[i],\n",
    "        \"Prediction\": apply_sort_order(corpus_predictions, sort_order)[i],\n",
    "        \"Example Importance\": result[i][0],\n",
    "    }\n",
    "    for i in range(len(corpus_dfs))\n",
    "]\n",
    "\n",
    "# Patient importances\n",
    "importance_dfs = [\n",
    "    pd.DataFrame(\n",
    "        result[j][2][idx - (corpus_time_steps_to_display - 1) : idx + 1].numpy(),\n",
    "        columns=[f\"{col}_fi\" for col in feature_names],\n",
    "    )\n",
    "    for j, idx in zip(range(len(result)), last_time_step_idx)\n",
    "]\n",
    "importance_data = [\n",
    "    {\n",
    "        \"importance_vals\": importance_dfs[i].transpose(),\n",
    "        \"Label\": apply_sort_order(corpus_targets, sort_order)[i],\n",
    "        \"Prediction\": apply_sort_order(corpus_predictions, sort_order)[i],\n",
    "        \"Example Importance\": result[i][0],\n",
    "    }\n",
    "    for i in range(len(corpus_dfs))\n",
    "]\n",
    "\n",
    "corpus_data = [\n",
    "    example\n",
    "    for example in corpus_data\n",
    "    if example[\"Example Importance\"] >= example_importance_threshold\n",
    "]\n",
    "importance_data = [\n",
    "    example\n",
    "    for example in importance_data\n",
    "    if example[\"Example Importance\"] >= example_importance_threshold\n",
    "]\n",
    "\n",
    "for example_i in range(len(corpus_data)):\n",
    "    if (last_time_step_idx[example_i] + 1) - corpus_time_steps_to_display < 0:\n",
    "        corpus_time_steps_to_display = last_time_step_idx[example_i] + 1\n",
    "    importance_df_colors = df_values_to_colors(\n",
    "        importance_data[example_i][\"importance_vals\"].copy()\n",
    "    )\n",
    "    importance_df_colors = importance_df_colors.applymap(\n",
    "        lambda x: f\"background-color: {x}\"\n",
    "    )\n",
    "    display_corpus_df = (\n",
    "        corpus_data[example_i][\"feature_vals\"]\n",
    "        .rename(\n",
    "            columns={\n",
    "                j: t_val\n",
    "                for j, t_val in enumerate(\n",
    "                    [\n",
    "                        f\"(t_max) - {i}\" if i != 0 else \"(t_max)\"\n",
    "                        for i in reversed(range(corpus_time_steps_to_display))\n",
    "                    ]\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        .style.apply(highlight, axis=None)\n",
    "    )\n",
    "    print(f\"Corpus Example: {example_i}\")\n",
    "    print(f\"Example Importance: {corpus_data[example_i]['Example Importance']}\")\n",
    "    display(display_corpus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SimplEx model\n",
    "explainer_path = f\"../demonstrator/resources/trained_models/RNN/Time series Prostate Cancer/simplex.pkl\"\n",
    "\n",
    "with open(explainer_path, \"wb\") as f:\n",
    "    print(f\"Saving SimplEx decomposition in {explainer_path}.\")\n",
    "\n",
    "    pkl.dump(simplex, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "cea5531e1b74877b862030d44e6f61698841614895721a2a93d09103bd0dddf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
